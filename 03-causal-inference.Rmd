---
output:
  pdf_document: default
  html_document: default
---
# Causual Inference

## Autoregression

A first-order autoregressive model (AR1) is defined as:

$$ y_t = \beta_0 + \beta_1y_{t-1} + \epsilon_t$$
The response variable in the previous time period ($y_t$) is the predictor. If we want to predict $y$ this year ($yt$) using measurements from two (rather than one) previous time steps, then we use a second order autoregressive model (AR2):

$$ y_t = \beta_0 + \beta_1y_{t-1} + \beta_2y_{t-2}+ \epsilon_t$$
More, generally, a $p-order$ autoregressive model is defined as:

$$ y_t = \beta_0 + \beta_1y_{t-1} + \beta_2y_{t-2}+...+ \beta_py_{t-p} + \epsilon_t$$

Let's use a AR2 to forecast an oscillating function.

```{r}

library(stats)

# Create a sinusoidal time series
t <- seq(0, 20, 0.1) # time axis
n <- length(t) # number of time steps
x <- sin(t)
time_series <- ts(x)

# Fit an autoregressive model using Ordinary Least Squares regression (OLS)
ar_model <- ar.ols(time_series, order.max = 2)
print(ar_model)
```

The Intercept refers to $\beta_0$ and the coefficients 1 and 2 refer to $\beta_1$ and $\beta_2$. Our AR2 is therefore:

$$ y_t = -0.0003167 + 1.99y_{t-1} - 1.00y_{t-2} $$
Let's plot this function

```{r}
# Set parameters
beta0 <- -0.0003167 # intercept
beta1 <- 1.99    # Coefficient of y_{t-1}
beta2 <- - 1.00 # Coefficient of y_{t-2}
sigma <- 1     # Standard deviation of the error term

# Initialize the time series
y <- numeric(n)
y3 <- time_series[3]
y2 <- time_series[2]
y[1:2] <- c(y2, y3)  # Initial value

# Generate the time series using the difference equation
for (t in 3:n) {
  epsilon_t <- 0  # Random error term
  y[t] <- beta0 + beta1 * y[t-1] + beta2 * y[t-2] + epsilon_t
}

# Plot the generated time series
plot(y, type = "l", col = "blue", lwd = 2, 
     xlab = "Time", ylab = "Y_t", main = "Time Series Generated by a Difference Equation")

```

Let's use the function to forecast our time series

```{r}
# Forecast the next 100 steps
forecast_length <- 100
pred <- stats::predict(ar_model, n.ahead = forecast_length)

# Combine actual and forecasted values
combined_series <- c(time_series, pred$pred)

# Plot the actual time series and the forecast
plot(
  combined_series,
  type = "l",
  col = "red",
  lwd = 2,
  xlab = "Time",
  ylab = "Value"
)

lines(1:n, time_series, col = "black", lwd = 2)  # Actual time series in black

# Add a legend
legend(
  "bottomleft",
  legend = c("Actual", "Forecast"),
  col = c("black", "red"),
  lwd = 2
)

```

## Vector autoregression (VAR)

Vector autoregression (VAR) extends the idea of autoregression to multivariate time series. Each variable is a linear function of past lags of itself and past lags of the other variables. Suppose we measure two different time series variables, denoted by $x_{t}$ and $y_{t}$. The first-order vector autoregressive model (VAR(1)) then is:

$$ x_{t} = \beta_{1} + \beta_{11}  x_{t-1} +  \beta_{12}  y_{t-1} + \epsilon_{1,t} $$
$$ y_{t} = \beta_{2} + \beta_{21}  x_{t-1} +  \beta_{22}  y_{t-1} + \epsilon_{2,t} $$

You can see that the variable $x_t$ depends on the 1-lag of itself ($x_{t-1}$) and of the other variable ($y_{t-1}$). As for AR, the order of VAR can be increased by including more time lags. Let's apply VAR to two time series. 

```{r}
library(vars)

set.seed(1234)

# Create time-series objects
ts1 <- ts(rnorm(n=5000))
ts2 <- ts(rnorm(n=5000))

# Bind time series
tsDat <- ts.union(ts1, ts2)

# Estimation of a VAR by utilising OLS per equation
tsVAR <- vars::VAR(tsDat, p = 3)

print(tsVAR)
```
The results contain the coefficients for both variables 

## Granger Causality
Does $x$ cause $y$? To address this question, we define two models. 

* (1) Restricted model: univariate autoregression of $y$: 

$$ y_{t} = \alpha_{0} + \alpha_{1}  y_{t-1} + \epsilon_{t} $$
* (2) Unrestricted model: multivariate autoregression of $y$:

$$ y_{t} = \beta_{0} + \beta_{1}  x_{t-1} +  \beta_{2}  y_{t-1} + u_{t} $$

The restricted model is an autoregressive model where $y_t$ depends only on $y$ lags. The unrestricted model is an autoregressive model where $y_t$ depends on $y$ and $x$ lags. If the unrestricted model outperforms the restricted model, then $x$ precedes $y$. The performance is measured via an F-test that compares the error terms of both models. 

```{r}

vars::causality(tsVAR, cause = "ts1")$Granger

```
You can see that there is no statistical significant relationship between both time series. Let's apply Granger for a test time series


```{r}

rm(list = ls())
library(vars)
# Create a sinusoidal time series
t <- seq(0, 20, 0.1) # time axis
n <- length(t) # number of time steps

x <- sin(t)
y <- x[1:(n-9)]
x <- x[10:n]

# Add noise to y
noise <- runif(n=length(y), min = -0.5, max = 0.5)
y <- y + noise

x <- ts(x)
y <- ts(y)

# (2) Create a vector autoregression model
tsDat <- ts.union(x, y)
plot(tsDat)
tsVAR <- vars::VAR(tsDat, p = 2)

# Apply Granger 
vars::causality(tsVAR, cause = "x")$Granger
vars::causality(tsVAR, cause = "y")$Granger

```


Now let's apply our new skills to the Lotka-Volterra model.

```{r}

rm(list = ls())
library(deSolve)

# (1) Create Predator-Prey time series
ode_function <- function(t, state, parameters)
{
  with(as.list(c(state)), {
    alpha <- 0.2
    ri <- 1
    m <- 0.2
    gamma <- 0.5
    K <- 10000
    
    dY <- ri * Y * (1 - Y / K) - alpha * R * Y
    dR <- alpha * gamma * R * Y - m * R
    
    list(c(dY, dR))
  })
}
state <- c(Y = 1, R = 1)
times <- seq(0, 70, 0.1)
out <- as.data.frame(ode(state, times, ode_function))

# (2) Create a vector autoregression model
predator <- ts(out$R)
prey <- ts(out$Y)
tsDat <- ts.union(predator, prey)
tsVAR <- vars::VAR(tsDat, p = 3)

# Apply Granger 
vars::causality(tsVAR, cause = "predator")$Granger
vars::causality(tsVAR, cause = "prey")$Granger

```
## Conduct Granger causality

Conduct your own Granger causality with abc data set 

```{r}


```

## State space reconstruction causality

```{r}
library(rEDM)

data(sardine_anchovy_sst)
data <- sardine_anchovy_sst

a <- ts(data)

plot(a)


df <- CCM( dataFrame = sardine_anchovy_sst, E = 3, Tp = 0, columns = "anchovy",
target = "np_sst", libSizes = "10 70 10", sample = 100 )

```

