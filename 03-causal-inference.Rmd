---
output:
  pdf_document: default
  html_document: default
---
# Causal Inference

Let's start by loading all the libraries that we will need

```{r, eval = FALSE}
library(stats)
library(vars)
library(deSolve)
```


## Autoregression

A first-order autoregressive model (AR1) is defined as:

$$ y_t = \beta_0 + \beta_1y_{t-1} + \epsilon_t$$
The response variable in the previous time period ($y_t$) is the predictor. If we want to predict $y$ this year ($y_t$) using measurements from two (rather than one) previous time steps, then we use a second order autoregressive model (AR2):

$$ y_t = \beta_0 + \beta_1y_{t-1} + \beta_2y_{t-2}+ \epsilon_t$$
More, generally, a $p$-order autoregressive model is defined as:

$$ y_t = \beta_0 + \beta_1y_{t-1} + \beta_2y_{t-2}+...+ \beta_py_{t-p} + \epsilon_t$$

Let's use a AR2 to forecast an oscillating function.

```{r}
# Create a sinusoidal time series
t <- seq(0, 20, 0.1) # time axis
n <- length(t) # number of time steps
x <- sin(t)
time_series <- ts(x)
plot(time_series)
```

Next, fit an second-order autoregressive model using Ordinary Least Squares regression (OLS)

```{r}
ar_model <- ar.ols(time_series, order.max = 2)
print(ar_model)
```

The Intercept refers to $\beta_0$ and the coefficients 1 and 2 refer to $\beta_1$ and $\beta_2$. Our AR2 is therefore:

$$ y_t = -0.0003167 + 1.99y_{t-1} - 1.00y_{t-2} $$

Let's plot this function:

* Set parameters

```{r}
beta0 <- -0.0003167 # intercept
beta1 <- 1.99    # Coefficient of y_{t-1}
beta2 <- - 1.00 # Coefficient of y_{t-2}
```

* Set the initial values of the time series
* We will need two initial values because we are predicting $y_t$ from $y_{t-1}$ and $y_{t-2}$

```{r}
y <- numeric(n)
y3 <- time_series[3]
y2 <- time_series[2]
y[1:2] <- c(y2, y3)  # Initial value
```

* Generate the time series using AR2

```{r}
for (t in 3:n) {
  epsilon_t <- 0  # Random error term
  y[t] <- beta0 + beta1 * y[t-1] + beta2 * y[t-2] + epsilon_t
}
```

* Plot the generated time series

```{r}
plot(y, type = "l", col = "blue", lwd = 2, 
     xlab = "Time", ylab = "Y_t", main = "Time Series Generated with AR2")
```

* You can reproduce a sinusoidal function using AR, amazing! Given that each $y_t$ depends on previous time steps (in our case $y_{t-1}$ and $y_{t-2}$), we should be able to use AR to preduct the future. Let's try that next using the ```stats::predict``` function.

```{r}
# Forecast the next 100 steps
forecast_length <- 100
pred <- stats::predict(ar_model, n.ahead = forecast_length)

# Combine actual and forecasted values
combined_series <- c(time_series, pred$pred)

# Plot the actual time series and the forecast
plot(
  combined_series,
  type = "l",
  col = "red",
  lwd = 2,
  xlab = "Time",
  ylab = "Value",
  main = "Autoregression with OLS"
)

lines(1:n, time_series, col = "black", lwd = 2)  # Actual time series in black

# Add a legend
legend(
  "bottomleft",
  legend = c("Actual", "Forecast"),
  col = c("black", "red"),
  lwd = 2
)

```

* The result matches what you intuitively expect. Let's now see how we can work with multiple variables. 


## Vector autoregression (VAR)

Vector autoregression (VAR) extends the idea of autoregression to multivariate time series. Each variable is a linear function of past lags of itself and past lags of the other variables. Suppose we measure two different time series variables, denoted by $x_{t}$ and $y_{t}$. The first-order vector autoregressive model (VAR(1)) then is:

$$ x_{t} = \beta_{1} + \beta_{11}  x_{t-1} +  \beta_{12}  y_{t-1} + \epsilon_{1,t} $$
$$ y_{t} = \beta_{2} + \beta_{21}  x_{t-1} +  \beta_{22}  y_{t-1} + \epsilon_{2,t} $$

You can see that the variable $x_t$ depends on the 1-lag of itself ($x_{t-1}$) and of the other variable ($y_{t-1}$). As for AR, the order of VAR can be increased by including more time lags. Let's apply VAR to two time series. 

```{r}

# Create time-series objects
ts1 <- ts(rnorm(n=5000))
ts2 <- ts(rnorm(n=5000))

# Bind time series and plot them
tsDat <- ts.union(ts1, ts2)
plot(tsDat)
```

* Calculate the coefficients for both time series using a time lag order of 1

```{r}
tsVAR <- vars::VAR(tsDat, p = 1) # p gives the lag order
print(tsVAR)
```

* The output gives you the coefficients and the y-intercept. The ```tsVAR``` object serves as an input when testing for Granger causality. But before we move on to Granger, let's brielfy recap what we have learned so far:

* Create a autoregression model (AR): ```stats::ar.ols(time_series, order.max = 2)```
* Predict the future using AR: ```stats::predict(ar_model, n.ahead = forecast_length)```
* Create a vector autoregression model (VAR): ```vars::VAR(tsDat, p = 3)```

Now we have the necessary skills for conducting causal inference, as explained next.

## Granger Causality

Using our time series above, we can now ask whether $x$ cause $y$. To address this question, we define two models. 

* (1) Restricted model: univariate autoregression of $y$: 

$$ y_{t} = \alpha_{0} + \alpha_{1}  y_{t-1} + \epsilon_{1,t} $$

* (2) Unrestricted model: multivariate autoregression of $y$:

$$ y_{t} = \beta_{0} + \beta_{1}  x_{t-1} +  \beta_{2}  y_{t-1} + \epsilon_{2,t} $$

The restricted model is an autoregressive model where $y_t$ depends only on $y$ lags. The unrestricted model is an autoregressive model where $y_t$ depends on $y$ and $x$ lags. If the unrestricted model outperforms the restricted model, then $x$ precedes $y$. The performance is measured via an $F$-test that compares the error terms of both models. 

* Conduct Granger causality:
```{r}
vars::causality(tsVAR, cause = "ts1")$Granger
```

* The null hypothesis ```H0``` says that the first time series does not Granger-cause thje second time series.
* The $p$-value exceeds 0.05, confirming the null hypothesis.
* You can safely assume that time series 1 does not Granger-cause time series 2
* Does that surprise you or did you expect this to happen? Explain why.
* Check for the reverse and see if time series 2 Granger-causes time series 1

```{r, eval = FALSE, echo = FALSE}
vars::causality(tsVAR, cause = "ts2")$Granger
```

Let's apply Granger causality to a different set of time series that do have a statistical association.

* Create two sinusoidal time series that differ by random noise and that are lagged

```{r}
rm(list = ls())
t <- seq(0, 20, 0.1) # time axis
n <- length(t) # number of time steps

x <- sin(t)
y <- x[1:(n-9)]
x <- x[10:n]

# Add noise to y
noise <- runif(n=length(y), min = -0.5, max = 0.5)
y <- y + noise

x <- ts(x)
y <- ts(y)

tsDat <- ts.union(x, y)
plot(tsDat)
```

* Create a vector autoregression model and apply Granger causation

```{r}
tsVAR <- vars::VAR(tsDat, p = 2)

# Apply Granger 
vars::causality(tsVAR, cause = "x")$Granger
vars::causality(tsVAR, cause = "y")$Granger

```

* What is your interpretation of the results?

Now let's apply our new skills to the Lotka-Volterra model.

* Create two time series, one for predator, the other for prey

```{r}

rm(list = ls())

# (1) Create Predator-Prey time series
ode_function <- function(t, state, parameters)
{
  with(as.list(c(state)), {
    alpha <- 0.2
    ri <- 1
    m <- 0.2
    gamma <- 0.5
    K <- 10000
    
    dY <- ri * Y * (1 - Y / K) - alpha * R * Y
    dR <- alpha * gamma * R * Y - m * R
    
    list(c(dY, dR))
  })
}
state <- c(Y = 1, R = 1)
times <- seq(0, 70, 0.1)
out <- as.data.frame(ode(state, times, ode_function))
predator <- ts(out$R)
prey <- ts(out$Y)
tsDat <- ts.union(predator, prey)
plot(tsDat)
```

* Conduct Granger causality

```{r}
tsVAR <- vars::VAR(tsDat, p = 3)
vars::causality(tsVAR, cause = "predator")$Granger
vars::causality(tsVAR, cause = "prey")$Granger

```
* Interpret the results

## Evaluate the statistical association between $x$, $y$, and $z$ in the Lorenz equations using Granger causality

To recall, the Lorenz equations were the first chaotic system to be discovered. They are three differential equations that were derived to represent idealized behavior of the earthâ€™s atmosphere:

$\frac{dx}{dt} = -\frac{8}{3}x+yz$

$\frac{dy}{dt} = -10 (y - z)$

$\frac{dz}{dt} = -xy + 28y-z$

* Good luck!

```{r, eval = FALSE, echo = FALSE}
Lorenz <- function(t, state, parameters)
{
  with(as.list(c(state)), {
    dx <- -8 / 3 * x + y * z
    dy <- -10 * (y - z)
    dz <- -x * y + 28 * y - z
    list(c(dx, dy, dz))
  })
}
state <- c(x = 1.01, y = 1, z = 1)
times <- seq(0, 50, 0.01)
out <- as.data.frame(ode(state, times, Lorenz, 0))

x <- ts(out$x)
y <- ts(out$y)
z <- ts(out$z)

tsDat <- ts.union(x,y,z)
plot(tsDat)
```

```{r, eval = FALSE, echo = FALSE}
tsVAR <- vars::VAR(tsDat, p = 3)
vars::causality(tsVAR, cause = "x")$Granger
vars::causality(tsVAR, cause = "y")$Granger
vars::causality(tsVAR, cause = "z")$Granger
```
